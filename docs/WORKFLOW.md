# Complete Workflow: From Seed Data to Deployment

Visual guide to the entire NanoDistill process.

## High-Level Flow

```
Your Seed Data (10 examples)
        â†“
        â”œâ”€â†’ [Download] Student Model (4GB, one-time)
        â”‚   â””â”€â†’ ~/.cache/huggingface/hub/
        â”‚
        â”œâ”€â†’ [Generate] CoT Traces (Claude API)
        â”‚   ğŸ“Š Input: 10 examples
        â”‚   ğŸ“Š Output: 10 reasoning traces
        â”‚
        â”œâ”€â†’ [Extract] Task Policy (Claude API)
        â”‚   ğŸ“Š Analyzes patterns in your examples
        â”‚
        â”œâ”€â†’ [Generate] Synthetic Examples (Claude API)
        â”‚   ğŸ“Š Input: 10 original examples
        â”‚   ğŸ“Š Output: 490 new diverse examples
        â”‚
        â”œâ”€â†’ [Merge] All Examples
        â”‚   ğŸ“Š 500 total examples with reasoning
        â”‚
        â”œâ”€â†’ [Train] Student Model (MLX-LM)
        â”‚   ğŸ“Š Fine-tunes Llama-3-8B
        â”‚   ğŸ“Š Uses LoRA for efficiency
        â”‚
        â””â”€â†’ Your Fine-Tuned Model âœ…
            â””â”€â†’ ./outputs/math-tutor-v1/
```

## Detailed Stage Breakdown

### Stage 1: Download Model (First Run Only)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ distill() starts                                â”‚
â”‚ â†“                                               â”‚
â”‚ Check: Is model cached?                         â”‚
â”‚ â”œâ”€ YES â†’ Skip download, use cached              â”‚
â”‚ â””â”€ NO  â†’ Download from HuggingFace              â”‚
â”‚         (4GB for Llama-3-8B-Instruct-4bit)     â”‚
â”‚         â†“                                       â”‚
â”‚         Save to ~/.cache/huggingface/hub/       â”‚
â”‚         â†“                                       â”‚
â”‚         âœ… Ready to use                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

First Run:  ~10-15 minutes (includes download)
Later Runs: < 1 minute (uses cache)
```

### Stage 2: Generate CoT Traces (Your Seeds)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Your 10 Seed Examples                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ {input: "What is 2+2?",  output: "4"}       â”‚
â”‚ {input: "What is 3+5?",  output: "8"}       â”‚
â”‚ ... (8 more examples)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
        [Send to Claude]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude Generates Reasoning                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input: "What is 2+2?"                        â”‚
â”‚ Thinking: "2 plus 2 equals 4"                â”‚
â”‚ Output: "4"                                  â”‚
â”‚                                              â”‚
â”‚ ... (for each of your 10 examples)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
        âœ… 10 Chain-of-Thought traces
        Saved to: traces_cot.jsonl
```

### Stage 3: Extract Task Policy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analyze Your 10 Examples + Reasoning         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What patterns do we see?                     â”‚
â”‚ â€¢ Task: Simple arithmetic                    â”‚
â”‚ â€¢ Input: Basic math questions                â”‚
â”‚ â€¢ Output: Numeric answers                    â”‚
â”‚ â€¢ Reasoning: Step-by-step calculation        â”‚
â”‚ â€¢ Difficulty: Beginner level                 â”‚
â”‚ â€¢ Key rules: Show all steps, be clear       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
        âœ… Task Policy Extracted
        (Used to guide synthetic generation)
```

### Stage 4: Generate Synthetic Examples

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude Generates 490 NEW Examples            â”‚
â”‚ (Constrained by extracted policy)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ "Generate examples following this pattern:"  â”‚
â”‚ â€¢ Task: Simple arithmetic                    â”‚
â”‚ â€¢ Input: Basic math questions                â”‚
â”‚ â€¢ Output: Numeric answers                    â”‚
â”‚ â€¢ Similar difficulty to seeds                â”‚
â”‚ â€¢ But NEW and DIVERSE                        â”‚
â”‚                                              â”‚
â”‚ Result examples:                             â”‚
â”‚ {input: "What is 12+8?", output: "20"}     â”‚
â”‚ {input: "What is 25-10?", output: "15"}    â”‚
â”‚ {input: "What is 3Ã—4?", output: "12"}      â”‚
â”‚ ... (487 more examples)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
        âœ… 490 Synthetic Examples Generated
```

### Stage 5: Generate CoT for Synthetic Data

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For Each Synthetic Example                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input: "What is 12+8?"                       â”‚
â”‚         â†“                                    â”‚
â”‚         [Claude generates reasoning]         â”‚
â”‚         â†“                                    â”‚
â”‚ Thinking: "12 + 8 = 20"                     â”‚
â”‚ Output: "20"                                 â”‚
â”‚                                              â”‚
â”‚ Repeat for all 490 synthetic examples        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
        âœ… 490 Synthetic Examples with CoT
```

### Stage 6: Merge & Prepare Training Data

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Combine All Training Data                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Your Seeds:          10 examples              â”‚
â”‚ Synthetic:          490 examples              â”‚
â”‚                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚ Total:              500 examples              â”‚
â”‚                                               â”‚
â”‚ Each example has:                             â”‚
â”‚ â€¢ Input (question)                            â”‚
â”‚ â€¢ Thinking (reasoning process)                â”‚
â”‚ â€¢ Output (answer)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
        âœ… 500 Training Examples Ready
```

### Stage 7: Train Student Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MLX-LM Fine-tunes Llama-3-8B                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Loading model...                               â”‚
â”‚ Configuring LoRA (parameter efficiency)       â”‚
â”‚ Training on 500 examples                      â”‚
â”‚ Hardware: Apple Silicon (MPS backend)         â”‚
â”‚ Epochs: 2                                      â”‚
â”‚ Batch size: auto-optimized by MLX             â”‚
â”‚                                               â”‚
â”‚ Progress:                                      â”‚
â”‚ Epoch 1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      ] Loss: 0.45      â”‚
â”‚ Epoch 2: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      ] Loss: 0.32      â”‚
â”‚                                               â”‚
â”‚ âœ… Training complete!                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
        âœ… Fine-tuned Model Saved
        ./outputs/math-tutor-v1/model/
```

## Complete Timeline

```
Time    Stage                          Duration   Cumulative
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0:00    Start                          -          0:00
0:00    Download model (first run)     10 min     10:00
0:10    Generate CoT traces            5 min      15:00
0:15    Extract policy                 2 min      17:00
0:17    Generate synthetic examples    5 min      22:00
0:22    Generate CoT for synthetic     3 min      25:00
0:25    Prepare training data          1 min      26:00
0:26    Train model                    3 min      29:00
0:29    Save model                     1 min      30:00
0:30    Done!                          âœ…         30:00

Notes:
- First run: ~30 minutes (includes 4GB download)
- Subsequent runs: ~20 minutes (cached model)
- All times on M1/M2 (M3 will be faster)
```

## File Structure After Completion

```
project/
â”œâ”€â”€ my_distillation.py          # Your script
â”œâ”€â”€ seeds.json                  # Your 10 examples
â”‚
â””â”€â”€ outputs/
    â””â”€â”€ math-tutor-v1/
        â”œâ”€â”€ model/              # â† FINAL MODEL (for inference)
        â”‚   â”œâ”€â”€ adapters.npz   # LoRA weights
        â”‚   â”œâ”€â”€ config.json
        â”‚   â”œâ”€â”€ tokenizer.json
        â”‚   â””â”€â”€ ...
        â”‚
        â”œâ”€â”€ traces_cot.jsonl    # Original 10 + reasoning
        â””â”€â”€ traces_amplified.jsonl  # All 500 training examples
```

## Usage After Training

### Option A: Quick Test (Local)

```
Your Trained Model
        â†“
    Load with MLX
        â†“
    Send prompt: "What is 7+8?"
        â†“
    Model generates response
        â†“
    Display: "The answer is 15"
```

### Option B: Serve with Ollama (Production)

```
Your Trained Model
        â†“
    Convert to GGUF format
        â†“
    Create Modelfile
        â†“
    Run: ollama create math-tutor -f Modelfile
        â†“
    Start server: ollama serve
        â†“
    Access via API or web interface
        â†“
    Query: curl http://localhost:11434/api/generate
```

### Option C: Web Application

```
Your Trained Model
        â†“
    Start MLX web server
        â†“
    Use REST API to query
        â†“
    Integrate into application
```

## Key Points

### Model Download
- âœ… Happens automatically on first use
- âœ… Only once (cached for reuse)
- âœ… No manual steps needed

### Training Process
- âœ… Uses your seed examples (10)
- âœ… Generates synthetic examples via Claude (490)
- âœ… Fine-tunes with MLX-LM on Apple Silicon
- âœ… Creates an efficient model (~5GB)

### After Training
- âœ… Model ready to use immediately
- âœ… Can test locally (MLX)
- âœ… Can deploy to production (Ollama)
- âœ… Can integrate into apps (API)

## Memory Usage Over Time

```
12GB â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚                      âœ“ Training
11GB â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚                      â”‚            âœ“ Model saved
10GB â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚  Downloaded model    â”‚
9GB  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚                      â”‚
8GB  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Final model uses 5GB
     â”‚                      â”‚
7GB  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚
6GB  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚      Training peaks
5GB  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       0min    5min    10min   15min   20min   25min
```

## Failure Recovery

```
If training is interrupted:

Interrupt at:              Recovery:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Download                   â†’ Resume from ~/.cache
CoT generation            â†’ Skip, regenerate
Policy extraction         â†’ Skip, regenerate
Synthetic generation      â†’ Skip, regenerate
CoT for synthetic          â†’ Skip, regenerate
Training                   â†’ Restart training
```

All data is saved, so you can resume!

---

**Ready to start?** See QUICK_START.md for step-by-step instructions.
