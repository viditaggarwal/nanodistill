"""Example: Bring Your Own Training Data.

Skip the CoT synthesis and amplification stages entirely by supplying
pre-formatted training data directly. No teacher API key is required.

Two approaches are shown below:
  1. Inline list of dicts
  2. JSONL file on disk
"""

import json
import tempfile
from pathlib import Path

from nanodistill import distill

# ---------------------------------------------------------------------------
# Option 1: Pass training data as a Python list
# ---------------------------------------------------------------------------
# Each record must have 'input', 'thinking', and 'output' fields.
# 'confidence' is optional (defaults to 0.9).

training_data = [
    {
        "input": "Translate 'hello' to French.",
        "thinking": (
            "The English word 'hello' is a common greeting. "
            "In French the standard translation is 'bonjour'."
        ),
        "output": "bonjour",
    },
    {
        "input": "Translate 'thank you' to French.",
        "thinking": (
            "'Thank you' is a polite expression of gratitude. " "The French equivalent is 'merci'."
        ),
        "output": "merci",
    },
    {
        "input": "Translate 'goodbye' to French.",
        "thinking": ("'Goodbye' is a parting greeting. " "In French this is 'au revoir'."),
        "output": "au revoir",
        "confidence": 0.95,
    },
    {
        "input": "Translate 'please' to French.",
        "thinking": (
            "'Please' is used to make polite requests. " "The French word is 's'il vous plait'."
        ),
        "output": "s'il vous plait",
    },
    {
        "input": "Translate 'yes' to French.",
        "thinking": ("'Yes' is an affirmative response. " "In French the word is 'oui'."),
        "output": "oui",
    },
]

result = distill(
    name="translator-from-list",
    training_data=training_data,
    student="mlx-community/Meta-Llama-3-8B-Instruct-4bit",
    output_dir="./outputs",
)

print(f"Model saved to: {result.model_path}")
print(f"Training examples: {result.metrics['training_examples']}")

# ---------------------------------------------------------------------------
# Option 2: Pass a path to a JSONL file
# ---------------------------------------------------------------------------
# Useful when you already have training data exported from another tool or
# generated by a separate script. One JSON object per line with the same
# required fields: input, thinking, output.

# Create a sample JSONL file for demonstration
# sample_jsonl = Path(tempfile.mkdtemp()) / "my_traces.jsonl"
# with open(sample_jsonl, "w") as f:
#     for record in training_data:
#         f.write(json.dumps(record) + "\n")
#
# print(f"\nSample JSONL written to: {sample_jsonl}")
#
# result = distill(
#     name="translator-from-file",
#     training_data=str(sample_jsonl),  # also accepts a Path object
#     student="mlx-community/Meta-Llama-3-8B-Instruct-4bit",
#     output_dir="./outputs",
# )
#
# print(f"Model saved to: {result.model_path}")
# print(f"Training examples: {result.metrics['training_examples']}")
